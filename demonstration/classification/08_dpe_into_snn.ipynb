{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from snn_dpe import Neuron, Synapse\n",
    "from snn_dpe.tools.data import normalize_iris_data, read_iris_data\n",
    "from snn_dpe.tools.network import (create_encoders, create_network,\n",
    "                                   reset_network, run_network)\n",
    "from snn_dpe.tools.plotting import plot_acc, plot_network\n",
    "from snn_dpe.tools.train import train_all\n",
    "\n",
    "iris_data_location = '../Data/Iris/iris.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpe_conv_search(args):\n",
    "    normalized_iris_data = args[0]\n",
    "    encoders = args[1]\n",
    "    classes = args[2]\n",
    "    labels = args[3]\n",
    "\n",
    "    sim_time = args[4]\n",
    "    window_size = args[5]\n",
    "    n_epochs = args[6]\n",
    "\n",
    "    #  create network and encoders\n",
    "    n_neurons = 16\n",
    "    n_synapses = int(n_neurons * np.random.uniform(low=2, high=3)) # random number from n_neurons * 2 to n_neurons * 3\n",
    "\n",
    "    \n",
    "    neurons = create_network(n_neurons, n_synapses)\n",
    "    dpe_weights = np.random.rand(n_neurons, len(classes))\n",
    "    E_t, avg_ss, c_acc = train_all(normalized_iris_data, labels, classes, neurons, encoders, dpe_weights, sim_time=sim_time, window_size=window_size, n_epochs=n_epochs)\n",
    "\n",
    "\n",
    "    output_neurons = []\n",
    "    best_copy = copy.deepcopy(neurons)\n",
    "\n",
    "    for i in range(3):\n",
    "        n = Neuron(i+4, 0.5, 0.0)\n",
    "\n",
    "        for j in range(4):\n",
    "            # if best_weights[j][i] > 0:\n",
    "            n1 = best_copy[j]\n",
    "            n2 = n\n",
    "\n",
    "            s = Synapse(n1, n2, dpe_weights[j][i])\n",
    "            n1.add_synapse(s)\n",
    "\n",
    "        output_neurons.append(n)\n",
    "\n",
    "    reset_network(neurons, encoders)\n",
    "\n",
    "    best_copy += output_neurons\n",
    "    # plot_network(best_copy)\n",
    "\n",
    "    # for n in best_copy:\n",
    "        # print(n.id)\n",
    "\n",
    "    sample = 6\n",
    "    n_correct = 0\n",
    "    for sample, label in zip(range(len(normalized_iris_data)),labels):\n",
    "        # feed a test sample into the test network\n",
    "        fires = run_network(best_copy, encoders, normalized_iris_data[sample], sim_time)\n",
    "\n",
    "        # plot_spikes(fires, attributes, normalized_iris_data[sample], sim_time)\n",
    "\n",
    "        sums = []\n",
    "        for f in fires:\n",
    "            sums.append(np.sum(len(f)))\n",
    "\n",
    "        # for e in encoders:\n",
    "        #     print(int(100/e.fire_period))\n",
    "\n",
    "        # print(sums[-3:])\n",
    "        pred = np.argmax(sums[-3:])\n",
    "        if pred == label:\n",
    "            n_correct += 1\n",
    "\n",
    "    return (n_correct, best_copy, dpe_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data, labels, classes, attributes = read_iris_data(iris_data_location, shuffle=True)\n",
    "\n",
    "normalized_iris_data = normalize_iris_data(iris_data, attributes)\n",
    "\n",
    "encoders = create_encoders(len(attributes))\n",
    "best_neurons = None\n",
    "best_weights = None\n",
    "best_E_t = None\n",
    "best_avg_ss = None\n",
    "best_c_acc = None\n",
    "max = 0\n",
    "args = []\n",
    "\n",
    "n_epochs = 10\n",
    "window_size = 10\n",
    "sim_time = 100\n",
    "\n",
    "\n",
    "for n in range(500):\n",
    "    args.append((normalized_iris_data, encoders, classes, labels, sim_time, window_size, n_epochs))\n",
    "    \n",
    "with Pool() as p:\n",
    "    res = p.map(dpe_conv_search, normalized_iris_data)\n",
    "\n",
    "for r in res:\n",
    "    if r[0] > max:\n",
    "        max = r[0]\n",
    "        best_neurons = r[1]\n",
    "        best_weights = r[2]\n",
    "\n",
    "max = float(max)/len(normalized_iris_data)\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune edges with small weights\n",
    "thresh = 0.2\n",
    "best_copy = copy.deepcopy(best_neurons)\n",
    "n_syn = 0\n",
    "\n",
    "for n in best_copy:\n",
    "    to_remove = []\n",
    "    for s in n.synapses:\n",
    "        n_syn += 1\n",
    "        if np.abs(s.weight) < thresh:\n",
    "            to_remove.append(s)\n",
    "\n",
    "    for s in to_remove:\n",
    "        n_syn -= 1\n",
    "        n.synapses.remove(s)\n",
    "        # print(s.weight)\n",
    "\n",
    "n_correct = 0\n",
    "for sample, label in zip(range(len(normalized_iris_data)),labels):\n",
    "    # feed a test sample into the test network\n",
    "    fires = run_network(best_copy, encoders, normalized_iris_data[sample], sim_time)\n",
    "\n",
    "    # plot_spikes(fires, attributes, normalized_iris_data[sample], sim_time)\n",
    "\n",
    "    sums = []\n",
    "    for f in fires:\n",
    "        sums.append(np.sum(len(f)))\n",
    "\n",
    "    # for e in encoders:\n",
    "    #     print(int(100/e.fire_period))\n",
    "\n",
    "    # print(sums[-3:])\n",
    "    pred = np.argmax(sums[-3:])\n",
    "    if pred == label:\n",
    "        n_correct += 1\n",
    "\n",
    "max = float(n_correct)/len(normalized_iris_data)\n",
    "print(max)\n",
    "print(n_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rand_search(arg):\n",
    "    n_neurons = np.random.randint(4, 7)\n",
    "    n_synapses = int(n_neurons * np.random.uniform(low=2, high=3)) # random number from n_neurons * 2 to n_neurons * 3\n",
    "\n",
    "    neurons = create_network(n_neurons, n_synapses, negative_weights=False, leak_range=(0.0, 0.0))\n",
    "\n",
    "    n_correct = 0\n",
    "    for sample, label in zip(range(len(normalized_iris_data)),labels):\n",
    "        # feed a test sample into the test network\n",
    "        fires = run_network(neurons, encoders, normalized_iris_data[sample], sim_time)\n",
    "\n",
    "        # plot_spikes(fires, attributes, normalized_iris_data[sample], sim_time)\n",
    "\n",
    "        sums = []\n",
    "        for f in fires:\n",
    "            sums.append(np.sum(len(f)))\n",
    "\n",
    "        # for e in encoders:\n",
    "        #     print(int(100/e.fire_period))\n",
    "\n",
    "        # print(sums[-3:])\n",
    "        pred = np.argmax(sums[-3:])\n",
    "        if pred == label:\n",
    "            n_correct += 1\n",
    "\n",
    "    return (n_correct, neurons)\n",
    "\n",
    "\n",
    "best_neurons = None\n",
    "max = 0\n",
    "arg = range(1000)\n",
    "\n",
    "with Pool() as p:\n",
    "    res = p.map(rand_search, arg)\n",
    "\n",
    "# print(f'{((max(res) / len(normalized_iris_data))*100):.1f} % Accuracy of frankenstein model')\n",
    "max = 0\n",
    "for r in res:\n",
    "    if r[0] > max:\n",
    "        max = r[0]\n",
    "print(max)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
